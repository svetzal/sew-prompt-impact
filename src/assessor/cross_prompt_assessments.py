import pathlib
import re
from collections import defaultdict

from mojentic.llm import MessageBuilder, LLMBroker

from assessor.output_processors.strip_thinking import strip_thinking


def generate_cross_prompt_assessments(llm: LLMBroker, folder_path: str, prompt_styles: list):
    """
    Generate a comparative assessment between different prompt styles across all models.

    Args:
        folder_path (str): Path to the folder containing output files
        prompt_styles (list): List of prompt styles to compare (e.g., ["plain", "fancy"])
    """
    folder = pathlib.Path(folder_path)

    # Ensure the folder exists
    if not folder.exists() or not folder.is_dir():
        raise ValueError(f"The path {folder_path} does not exist or is not a directory")

    # Dictionary to store output files for each model and prompt style
    model_outputs = defaultdict(lambda: defaultdict(list))

    # Find all output files and organize them by model and prompt style
    for file_path in folder.iterdir():
        if file_path.is_dir() or file_path.suffix.lower() != '.md' or "assessment" in str(
                file_path):
            continue

        if "output" in str(file_path):
            # Extract model name and prompt style from filename
            file_name = file_path.stem

            # Extract style names from output filenames
            # Output filenames are derived from prompt filenames, which follow the pattern
            # "prompt-{style}.md"
            # So we need to extract the style from the output filename
            style_match = None
            for style in prompt_styles:
                if f"prompt-{style}" in file_name:
                    style_match = style
                    break

            if not style_match:
                continue

            prompt_style = style_match

            # Extract model name (between "output-" and end of filename)
            model_match = re.search(r'output-(.*)', file_name)
            if model_match:
                model_name = model_match.group(1)
                model_outputs[model_name][prompt_style].append(file_path)

    # For each model, generate a comparative assessment between prompt styles
    for model_name, style_outputs in model_outputs.items():
        # Only generate assessment if we have outputs for all prompt styles
        if all(style in style_outputs for style in prompt_styles):
            # Create assessment prompt
            assessment_prompt = f"""
            Please compare the outputs generated by the {model_name} model for different prompt 
            styles ({", ".join(prompt_styles)}).

            First, provide a tabular super-condensed comparison of the prompt styles, 
            highlighting key differences and strengths/weaknesses of each style. Format this as a 
            markdown table.

            Then, provide a more extensive qualitative analysis focusing on these questions:
            1. Which prompt style gives the best results overall?
            2. What aspects of the model's response differ between the different prompt styles?
            3. What aspects of the model's response are consistent across all prompt styles?

            In your assessment, refer to each output by its prompt style (e.g., 
{", ".join(f'"{style}"' for style in prompt_styles)}).
            """

            # Create message builder
            mb = MessageBuilder(assessment_prompt)

            # Add source files for each prompt style
            for style in prompt_styles:
                for output_file in style_outputs[style]:
                    # Find the original prompt file
                    # Output filename is derived from prompt filename, which follows the pattern
                    # "prompt-{style}.md"
                    prompt_file_path = folder / f"prompt-{style}.md"

                    if prompt_file_path.exists():
                        mb.add_file(prompt_file_path)

                    # Add the output file
                    mb.add_file(output_file)

            # Generate assessment
            assessment = llm.generate(messages=[mb.build()])

            # Strip out thinking text
            assessment = strip_thinking(assessment)

            # Create assessment file path
            assessment_file_path = folder / f"cross-prompt-assessment-{model_name}.md"
            with open(assessment_file_path, 'w') as assessment_file:
                assessment_file.write(assessment)

            print(
                f"Created cross-prompt assessment for {model_name} -> {assessment_file_path.name}")

"""
Assessment generation utilities for the assessor module.
"""

import re
from collections import defaultdict
from pathlib import Path
from typing import List, Optional, Union

from mojentic.llm import MessageBuilder

from assessor.config import default_config, Config
from assessor.file_gateway import FileGateway
from assessor.llm_handler import get_assessment_llm
from assessor.utils import strip_thinking


def generate_assessment(
    source_file: Union[str, Path], 
    output_files: List[Union[str, Path]],
    config: Optional[Config] = None,
    file_gateway: Optional[FileGateway] = None
):
    """
    Generate an assessment for a source file and its outputs.

    Args:
        source_file: Path to the source file
        output_files: List of paths to output files
        config: Optional Config instance (defaults to default_config)
        file_gateway: Optional FileGateway instance (defaults to a new instance)

    Returns:
        str: The assessment text
    """
    if not output_files:
        return None

    # Use provided config or default
    config = config or default_config

    # Use provided file gateway or create a new one
    file_gateway = file_gateway or FileGateway()

    # Create assessment prompt
    assessment_prompt = f"""
    Please assess the quality and differences between the following outputs generated for the 
    source document '{Path(source_file).name}'.
    In the assessment refer to each output by its filename.
    """

    mb = MessageBuilder(assessment_prompt)
    mb.add_file(source_file)
    mb.add_files(*output_files)

    llm = get_assessment_llm(config)
    assessment = llm.generate(messages=[mb.build()])

    # Strip out thinking text
    assessment = strip_thinking(assessment)

    return assessment

def generate_cross_prompt_assessment(
    folder_path: str, 
    prompt_styles: List[str],
    config: Optional[Config] = None,
    file_gateway: Optional[FileGateway] = None
):
    """
    Generate a comparative assessment between different prompt styles across all models.

    Args:
        folder_path: Path to the folder containing output files
        prompt_styles: List of prompt styles to compare (e.g., ["plain", "fancy"])
        config: Optional Config instance (defaults to default_config)
        file_gateway: Optional FileGateway instance (defaults to a new instance)

    Returns:
        dict: Dictionary mapping model names to their assessment file paths
    """
    # Use provided config or default
    config = config or default_config

    # Use provided file gateway or create a new one
    file_gateway = file_gateway or FileGateway()

    folder = Path(folder_path)
    assessment_files = {}

    # Ensure the folder exists
    if not file_gateway.folder_exists(folder):
        raise ValueError(f"The path {folder_path} does not exist or is not a directory")

    # Dictionary to store output files for each model and prompt style
    model_outputs = defaultdict(lambda: defaultdict(list))

    # Find all output files and organize them by model and prompt style
    for file_path in file_gateway.list_files_with_pattern(
        folder, 
        suffix='.md', 
        exclude_patterns=["assessment"]
    ):
        if "output" in str(file_path):
            # Extract model name and prompt style from filename
            file_name = file_path.stem

            # Extract style names from output filenames
            # Output filenames are derived from prompt filenames, which follow the pattern "prompt-{style}.md"
            # So we need to extract the style from the output filename
            style_match = None
            for style in prompt_styles:
                if f"prompt-{style}" in file_name:
                    style_match = style
                    break

            if not style_match:
                continue

            prompt_style = style_match

            # Extract model name (between "output-" and end of filename)
            model_match = re.search(r'output-(.*)', file_name)
            if model_match:
                model_name = model_match.group(1)
                model_outputs[model_name][prompt_style].append(file_path)

    # For each model, generate a comparative assessment between prompt styles
    for model_name, style_outputs in model_outputs.items():
        # Only generate assessment if we have outputs for all prompt styles
        if all(style in style_outputs for style in prompt_styles):
            # Create assessment prompt
            assessment_prompt = f"""
            Please compare the outputs generated by the {model_name} model for different prompt styles ({", ".join(prompt_styles)}).

            First, provide a tabular super-condensed comparison of the prompt styles, highlighting key differences and strengths/weaknesses of each style. Format this as a markdown table.

            Then, provide a more extensive qualitative analysis focusing on these questions:
            1. Which prompt style gives the best results overall?
            2. What aspects of the model's response differ between the different prompt styles?
            3. What aspects of the model's response are consistent across all prompt styles?

            In your assessment, refer to each output by its prompt style (e.g., {", ".join(f'"{style}"' for style in prompt_styles)}).
            """

            # Create message builder
            mb = MessageBuilder(assessment_prompt)

            # Add source files for each prompt style
            for style in prompt_styles:
                for output_file in style_outputs[style]:
                    # Find the original prompt file
                    # Output filename is derived from prompt filename, which follows the pattern "prompt-{style}.md"
                    prompt_file_path = folder / f"prompt-{style}.md"

                    if file_gateway.file_exists(prompt_file_path):
                        mb.add_file(prompt_file_path)

                    # Add the output file
                    mb.add_file(output_file)

            # Generate assessment
            llm = get_assessment_llm(config)
            assessment = llm.generate(messages=[mb.build()])

            # Strip out thinking text
            assessment = strip_thinking(assessment)

            # Create assessment file path
            assessment_file_path = folder / f"cross-prompt-assessment-{model_name}.md"
            file_gateway.write_file(assessment_file_path, assessment)

            assessment_files[model_name] = assessment_file_path

    return assessment_files

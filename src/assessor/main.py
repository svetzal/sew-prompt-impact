"""
Prompt Impact Assessment Tool

This script processes prompt files with various LLM models and generates assessments.
It supports comparing different prompt styles (e.g., plain vs fancy) across multiple models
to answer questions like:
1. "Does this fancier prompt give me better results than the plain one?"
2. "What models respond best to the directives in the fancier prompt?"

Usage:
    python main.py [options]

Options:
    --folder FOLDER     Folder containing prompt files (default: 'prompts')
    --openai            Use OpenAI models (default: True)
    --ollama            Use Ollama models (default: True)
    --prompt STYLE      Filter prompts by style name (e.g., "plain" or "fancy")
                        Files are expected to follow the pattern "prompt-{style}.md"
    --compare STYLES    Generate cross-prompt assessments for specified prompt styles
                        Example: --compare plain fancy

Example usage:
    # Process all prompts with both OpenAI and Ollama models
    python main.py

    # Process only plain prompts with OpenAI models
    python main.py --prompt plain --ollama False

    # Compare plain and fancy prompts across all models
    python main.py --compare plain fancy
"""

import os
import pathlib
import sys
import re
import argparse
from collections import defaultdict

from mojentic.llm import LLMBroker, MessageBuilder
from mojentic.llm.gateways import OpenAIGateway, OllamaGateway
from mojentic.llm.gateways.models import LLMMessage

openai = OpenAIGateway(api_key=os.getenv("OPENAI_API_KEY"))
ollama = OllamaGateway()


def strip_thinking(text):
    """
    Strip out thinking text enclosed in <think>...</think> tags from the response.

    Args:
        text (str): The text to process

    Returns:
        str: The text with thinking sections removed
    """
    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)


def generate_cross_prompt_assessment(folder_path: str, prompt_styles: list):
    """
    Generate a comparative assessment between different prompt styles across all models.

    Args:
        folder_path (str): Path to the folder containing output files
        prompt_styles (list): List of prompt styles to compare (e.g., ["plain", "fancy"])
    """
    folder = pathlib.Path(folder_path)

    # Ensure the folder exists
    if not folder.exists() or not folder.is_dir():
        raise ValueError(f"The path {folder_path} does not exist or is not a directory")

    # Dictionary to store output files for each model and prompt style
    model_outputs = defaultdict(lambda: defaultdict(list))

    # Find all output files and organize them by model and prompt style
    for file_path in folder.iterdir():
        if file_path.is_dir() or file_path.suffix.lower() != '.md' or "assessment" in str(file_path):
            continue

        if "output" in str(file_path):
            # Extract model name and prompt style from filename
            file_name = file_path.stem

            # Extract style names from output filenames
            # Output filenames are derived from prompt filenames, which follow the pattern "prompt-{style}.md"
            # So we need to extract the style from the output filename
            style_match = None
            for style in prompt_styles:
                if f"prompt-{style}" in file_name:
                    style_match = style
                    break

            if not style_match:
                continue

            prompt_style = style_match

            # Extract model name (between "output-" and end of filename)
            model_match = re.search(r'output-(.*)', file_name)
            if model_match:
                model_name = model_match.group(1)
                model_outputs[model_name][prompt_style].append(file_path)

    # For each model, generate a comparative assessment between prompt styles
    for model_name, style_outputs in model_outputs.items():
        # Only generate assessment if we have outputs for all prompt styles
        if all(style in style_outputs for style in prompt_styles):
            # Create assessment prompt
            assessment_prompt = f"""
            Please compare the outputs generated by the {model_name} model for different prompt styles ({", ".join(prompt_styles)}).

            First, provide a tabular super-condensed comparison of the prompt styles, highlighting key differences and strengths/weaknesses of each style. Format this as a markdown table.

            Then, provide a more extensive qualitative analysis focusing on these questions:
            1. Which prompt style gives the best results overall?
            2. What aspects of the model's response differ between the different prompt styles?
            3. What aspects of the model's response are consistent across all prompt styles?

            In your assessment, refer to each output by its prompt style (e.g., {", ".join(f'"{style}"' for style in prompt_styles)}).
            """

            # Create message builder
            mb = MessageBuilder(assessment_prompt)

            # Add source files for each prompt style
            for style in prompt_styles:
                for output_file in style_outputs[style]:
                    # Find the original prompt file
                    # Output filename is derived from prompt filename, which follows the pattern "prompt-{style}.md"
                    prompt_file_path = folder / f"prompt-{style}.md"

                    if prompt_file_path.exists():
                        mb.add_file(prompt_file_path)

                    # Add the output file
                    mb.add_file(output_file)

            # Generate assessment
            llm = LLMBroker(model="o1", gateway=openai)
            assessment = llm.generate(messages=[mb.build()])

            # Strip out thinking text
            assessment = strip_thinking(assessment)

            # Create assessment file path
            assessment_file_path = folder / f"cross-prompt-assessment-{model_name}.md"
            with open(assessment_file_path, 'w') as assessment_file:
                assessment_file.write(assessment)

            print(f"Created cross-prompt assessment for {model_name} -> {assessment_file_path.name}")


def get_available_prompt_styles(folder_path: str):
    """
    Extract all available prompt styles from the folder.

    Args:
        folder_path (str): Path to the folder containing prompt files

    Returns:
        list: List of available prompt styles
    """
    folder = pathlib.Path(folder_path)
    styles = []

    for file_path in folder.iterdir():
        if (file_path.is_dir() or 
                file_path.suffix.lower() != '.md' or 
                "output" in str(file_path) or 
                "assessment" in str(file_path)):
            continue

        # Extract style from prompt filename (pattern: "prompt-{style}.md")
        match = re.match(r'prompt-(.+)\.md$', file_path.name)
        if match:
            styles.append(match.group(1))

    return styles

def process_folder(folder_path: str, use_openai: bool = True, use_ollama: bool = True, prompt_pattern: str = None):
    """
    Process all files in the given folder:
    1. Read each file's contents
    2. Send the contents to the LLM (both OpenAI and Ollama models if specified)
    3. Write the output to a new file with "-output" suffix
    4. After all outputs are created, assess them using the o1 model
    5. Write the assessment to a new file with "-assessment" suffix

    Args:
        folder_path (str): Path to the folder containing files to process
        use_openai (bool): Whether to use OpenAI models
        use_ollama (bool): Whether to use Ollama models
        prompt_pattern (str): Optional comma-separated list of style names to filter prompt files (e.g., "plain,fancy").
                             Files are expected to follow the pattern "prompt-{style}.md"
    """
    folder = pathlib.Path(folder_path)

    # Ensure the folder exists
    if not folder.exists() or not folder.is_dir():
        raise ValueError(f"The path {folder_path} does not exist or is not a directory")

    # Dictionary to store output files for each source document
    output_files = defaultdict(list)

    # Define models
    openai_models = ["gpt-4o", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano", "o3-mini", "o4-mini"]
    ollama_models = ["qwen3:32b", "qwen3:30b", "qwen3:30b-a3b-q4_K_M", "qwen2.5:32b", "qwen2.5-coder:32b", "qwen2.5-coder:7b", "qwen2.5:72b", "llama3.3-70b-32k"]
    # models = ["gpt-4o", "gpt-4.1-nano"]

    # Process OpenAI models if specified
    if use_openai:
        for llm_model_name in openai_models:
            llm = LLMBroker(model=llm_model_name, gateway=openai)

            # Process each file in the folder
            for file_path in folder.iterdir():
                # Skip directories, non-markdown files, and output/assessment files
                if (file_path.is_dir() or
                        file_path.suffix.lower() != '.md' or
                        "output" in str(file_path) or
                        "assessment" in str(file_path)):
                    continue

                # If prompt_pattern is provided, check if the file matches any of the specified styles
                if prompt_pattern:
                    styles = [style.strip() for style in prompt_pattern.split(',')]
                    if not any(file_path.stem == f"prompt-{style}" for style in styles):
                        continue

                # Read the file contents
                with open(file_path, 'r') as file:
                    file_contents = file.read()

                # Create an LLMMessage with the file contents
                message = LLMMessage(content=file_contents)

                # Send the message to the LLM
                response = llm.generate(messages=[message])

                # Strip out thinking text
                response = strip_thinking(response)

                # Create the output file path
                output_file_path = file_path.with_name(
                    f"{file_path.stem}-output-{llm_model_name.replace(':','-')}{file_path.suffix}")

                # Write the response to the output file
                with open(output_file_path, 'w') as output_file:
                    output_file.write(response)

                # Store the output file path for later assessment
                output_files[file_path].append(output_file_path)

                print(f"Processed {file_path.name} -> {output_file_path.name}")

    # Process Ollama models if specified
    if use_ollama:
        for llm_model_name in ollama_models:
            llm = LLMBroker(model=llm_model_name, gateway=ollama)

            # Process each file in the folder
            for file_path in folder.iterdir():
                # Skip directories, non-markdown files, and output/assessment files
                if (file_path.is_dir() or
                        file_path.suffix.lower() != '.md' or
                        "output" in str(file_path) or
                        "assessment" in str(file_path)):
                    continue

                # If prompt_pattern is provided, check if the file matches any of the specified styles
                if prompt_pattern:
                    styles = [style.strip() for style in prompt_pattern.split(',')]
                    if not any(file_path.stem == f"prompt-{style}" for style in styles):
                        continue

                # Read the file contents
                with open(file_path, 'r') as file:
                    file_contents = file.read()

                # Create an LLMMessage with the file contents
                message = LLMMessage(content=file_contents)

                # Send the message to the LLM
                response = llm.generate(messages=[message])

                # Strip out thinking text
                response = strip_thinking(response)

                # Create the output file path
                output_file_path = file_path.with_name(
                    f"{file_path.stem}-output-{llm_model_name.replace(':','-')}{file_path.suffix}")

                # Write the response to the output file
                with open(output_file_path, 'w') as output_file:
                    output_file.write(response)

                # Store the output file path for later assessment
                output_files[file_path].append(output_file_path)

                print(f"Processed {file_path.name} -> {output_file_path.name}")

    # Assess all outputs using o1-pro model
    llm = LLMBroker(model="o1", gateway=openai)

    for source_file, outputs in output_files.items():
        if not outputs:
            continue

        # Create assessment prompt
        assessment_prompt = f"""
        Please assess the quality and differences between the following outputs generated for the 
        source document '{source_file.name}'.
        In the assessment refer to each output by its filename.
        """

        mb = MessageBuilder(assessment_prompt)
        mb.add_file(source_file)
        mb.add_files(*outputs)

        assessment = llm.generate(messages=[mb.build()])

        # Strip out thinking text
        assessment = strip_thinking(assessment)

        assessment_file_path = source_file.with_name(
            f"{source_file.stem}-assessment{source_file.suffix}")
        with open(assessment_file_path, 'w') as assessment_file:
            assessment_file.write(assessment)

        print(f"Created assessment for {source_file.name} -> {assessment_file_path.name}")


if __name__ == "__main__":
    # Set up argument parser
    parser = argparse.ArgumentParser(description='Process prompts with LLMs and generate assessments')
    parser.add_argument('--folder', type=str, default='prompts', help='Folder containing prompt files')
    parser.add_argument('--openai', action='store_true', default=True, help='Use OpenAI models')
    parser.add_argument('--ollama', action='store_true', default=True, help='Use Ollama models')
    parser.add_argument('--prompt', type=str, help='Filter prompts by comma-separated style names (e.g., "plain,fancy"). Files are expected to follow the pattern "prompt-{style}.md"')
    parser.add_argument('--compare', nargs='+', help='Generate cross-prompt assessments for specified prompt styles')

    args = parser.parse_args()

    # Process prompts with LLMs
    process_folder(folder_path=args.folder, use_openai=args.openai, use_ollama=args.ollama, prompt_pattern=args.prompt)
    print(f"Successfully processed files in {args.folder}")

    # Determine which prompt styles to compare
    styles_to_compare = []

    if args.compare:
        # Use explicitly specified styles
        styles_to_compare = args.compare
    elif args.prompt:
        # Use styles from the prompt filter
        styles_to_compare = [style.strip() for style in args.prompt.split(',')]
    else:
        # Use all available styles
        styles_to_compare = get_available_prompt_styles(args.folder)

    # Ensure we have at least two styles to compare
    if len(styles_to_compare) < 2:
        print("Error: At least two prompt styles are required for comparison.")
        sys.exit(1)

    # Generate cross-prompt assessments
    print(f"Generating cross-prompt assessments for styles: {', '.join(styles_to_compare)}")
    generate_cross_prompt_assessment(folder_path=args.folder, prompt_styles=styles_to_compare)
    print("Cross-prompt assessments completed")
